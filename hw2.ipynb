{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# HW 2 \n",
    "\n",
    "This homework notebook has been adapted from the PyTorch tutorial [Text Classification with the TorchText Library](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html).\n",
    "\n",
    "[Torchtext](https://pytorch.org/text/stable/index.html) is a library within the PyTorch framework that consists of data processing utilities and popular datasets for natural language processing.\n",
    "\n",
    "In this homework, we will\n",
    "- build a logistic regression model for text classification using bag of words (BoW).\n",
    "- extend the above model to use continuous bag of words (CBoW).\n",
    "- consider some other extensions, such as using a better version of gradient descent.\n",
    "\n",
    "You have to complete **13 tasks**, specified at appropriate places, worth a total of 70 points.  Most of them require writing some code.  Please add your code (or written answers) to this notebook and submit it as part of your homework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "Below is a guide to set up a Python environment for this assignment.  We recommend installing and using [Anaconda](https://www.anaconda.com/) for environment management for Python.\n",
    "After [Anaconda is installed](https://docs.anaconda.com/anaconda/install/index.html), \n",
    "one may use the commands below to create and activate the enviroment.\n",
    "```\n",
    "conda create --name capp30255 python=3.10\n",
    "conda activate capp30255\n",
    "```\n",
    "\n",
    "PyTorch and particularly NLP libraries such as [torchtext](https://pytorch.org/text/stable/index.html) are evolving rapidly. In our experience code written for a version of the library often does not work as new versions are released in a few months.  So it is important that we use the same version.  This homework has been tested on PyTorch 2.0 and torchtext 0.15.1.\n",
    "\n",
    "[Install PyTorch](https://pytorch.org/get-started/locally/) following specific instructions according to the platforms.  For example, one may run `conda install pytorch=2.0 -c pytorch` on a Mac, and `conda install pytorch=2.0 cpuonly -c pytorch` on a Windows or a GNU/Linux (if no GPU is to be used).\n",
    "\n",
    "Next install jupyter and matplotlib\n",
    "```\n",
    "conda install jupyter\n",
    "conda install matplotlib -c conda-forge\n",
    "```\n",
    "\n",
    "Finally, we also need to install some other dependencies. Conda doesn't seem to have the latest versions, so it is better to use ``pip``.\n",
    "\n",
    "```\n",
    "pip install torchtext==0.15.1\n",
    "pip install torchdata==0.6.0\n",
    "pip install portalocker==2.7.0\n",
    "```\n",
    "\n",
    "Remember to run the jupyter notebook with the kernel `capp30255` as named above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torchdata\n",
    "import portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ensure the correct versions are installed.\n",
    "def check_lib_versions():\n",
    "    libversions = {torch: \"2.0\",  torchtext: \"0.15.1\", torchdata: \"0.6.0\",\n",
    "                   portalocker: \"2.7.0\"}\n",
    "    for l, v in libversions.items():\n",
    "        try:\n",
    "            assert l.__version__ == v\n",
    "        except:\n",
    "            name = [n for n in globals() if globals()[n] is l][0]\n",
    "            print(f'Error: The version of {name} should be {v}.')\n",
    "check_lib_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes Dataset and DataLoader\n",
    "\n",
    "Dataset and DataLoader are PyTorch classes that provides utilities for iterating through and sampling from a dataset. They provide several features for advanced applications (e.g., skim through [this tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) on writing custom datasets and dataloaders).\n",
    "\n",
    "We'll work with the `` AG_NEWS`` dataset included within torchtext, and will write a custom dataloader to create minibatches of examples for training and testing.  The [``AG_NEWS``](https://rdrr.io/cran/textdata/man/dataset_ag_news.html) consists of about 120,000 examples of text from news sources, each labeled with one of 4 classes (world, sports, business, science and technology). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset is an iterable. It returns an iterator via an ``iter()`` call. When called in a for loop or using next it returns a sequence of examples, each of datapoint is a pair of label and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_data)\n",
    "example1 = next(train_iter)\n",
    "example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: tokenizing, converting to BoW\n",
    "\n",
    "\n",
    "In order to convert a piece of text into, say, a BoW vector, we need to do\n",
    "\n",
    "- Split up the text into a sequence of words or tokens.  This can be a suprisingly complex task because of various uses of apostrophe, uses of contractions, etc.  (E.g., see this [article](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html) at Stanford.)  We'll simply use a tokenizer provided by torchtext.\n",
    "\n",
    "- Determine which tokens will be included in our BoW vector, and which will be ignored.  Using too few will degrade prediction performance, while using too many will slow down the system.  We'll only include words that occur in the training set at least a given minimum number of times.\n",
    "\n",
    "- Convert each included word into an numerical index corresponding to its position on the BoW vectors. Torchtext provide a ``Vocab`` class to help with this. (Alternately, we can convert each word into a dense vector representation using pretrained representations such as [GloVe](https://nlp.stanford.edu/projects/glove/).) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(train_iter):\n",
    "    for _, text in train_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only inlcude words that occur at least 1000 times in the training data.\n",
    "# Also let \"<unk>\" represent unknown words, i.e., words not in the vocabulary.\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_iter), specials=[\"<unk>\"], min_freq=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [documentation](https://pytorch.org/text/stable/vocab.html) for ``Vocab.``  Now write code to answer the following:\n",
    "- **Task 1** [2 points]: Print the number of words in ``vocab``.\n",
    "- **Task 2** [2]: Print the index of the work \"economy\".\n",
    "- **Task 3** [2]: Print the word at index 500.\n",
    "- **Task 4** [2]: Find out what index vocab has for the speci\n",
    "al unknown token `\"<unk>\"`, and set it as the default index of `vocab`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the indices corresponding to the text in example1\n",
    "print(example1[1])\n",
    "print([vocab[token] for token in tokenizer(example1[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [dataloader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) takes a dataset and creates minibatches of examples, each of a specified batch size.  We specify how it processes each example from the dataset via a custom collate function.\n",
    "\n",
    "**Task 5** [10]: Write a collate function ``collate_into_bow`` that accepts a batch of k examples created from the dataset above and returns two tensors:\n",
    "a tensor of shape (k,) containing the labels of the batch, and a tensor of shape (k, m), in which m is the number of tokens in the vocabulary, containing the bow vectors for the examples. Further:\n",
    "\n",
    "1. The labels in the dataset are numbers 1 to 4. Since PyTorch is 0-indexed, please convert them to numbers 0 to 3 in the collate function.\n",
    "2. Remember that the entry in each bow vector is the **relative frequency** of the word in the corresponding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_into_bow(batch):\n",
    "    ## WRITE YOUR CODE HERE    \n",
    "\n",
    "\n",
    "def test_collate():\n",
    "    w1 = vocab.lookup_token(3)\n",
    "    w2 = vocab.lookup_token(7)\n",
    "    w3 = vocab.lookup_token(8)\n",
    "    w4 = vocab.lookup_token(9)\n",
    "    examples = [\n",
    "        (1, \" \".join([w1, w2, w3, w4])),\n",
    "        (2, \" \".join([w2, w1, w3, w4])),\n",
    "        (4, \" \".join([w4, w2, w3, w4])),\n",
    "        (3, \" \".join([w2, w2, w2, w4])),\n",
    "        (3, \" \".join([w1, w2])),\n",
    "\n",
    "    ]\n",
    "    bowt = torch.tensor(\n",
    "        [\n",
    "            [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.25, 0.25],\n",
    "            [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.25, 0.25],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.50],\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.25],\n",
    "            [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.00],\n",
    "\n",
    "        ]) \n",
    "    lt, tt = collate_into_bow(examples)\n",
    "    assert lt.shape == torch.Size([5])\n",
    "    assert tt.shape == torch.Size([5, len(vocab)])\n",
    "    assert torch.equal(lt, torch.tensor([0, 1, 3, 2, 2]))\n",
    "    assert torch.equal(tt[:,:10], bowt)\n",
    "    assert tt[:,10:].sum().item() == 0.00\n",
    "    print('Test passed.')\n",
    "    \n",
    "test_collate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collate function is provided to a dataloader as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(train_iter, batch_size=16, shuffle=False, \n",
    "                        collate_fn=collate_into_bow)\n",
    "for idx, (lt, tt) in enumerate(dataloader):\n",
    "    print(idx, lt.shape, tt.shape)\n",
    "    if idx == 4: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A BoW Classifier Class\n",
    "\n",
    "**Task 6** [5]: Write a BoWClassifier class with one single linear layer,\n",
    "similar to the one in [Robert Guthrie's tutorial](https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#sphx-glr-beginner-nlp-deep-learning-tutorial-py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    ## WRITE YOUR CODE BELOW\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a model object of the class BoWClassifier.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = AG_NEWS(split='train')\n",
    "num_labels = len(set([label for (label, text) in train_data]))\n",
    "vocab_size = len(vocab)\n",
    "model = BoWClassifier(num_labels, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an epoch\n",
    "\n",
    "The code below is similar to what we saw in Gutherie's tutorial. It prints the loss every 500 iterations. ``model.train()`` is used by PyTorch to set the model in training model.  This usually only impacts some advanced architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "\n",
    "def train_an_epoch(dataloader, optimizer):\n",
    "    model.train() # Sets the module in training mode.\n",
    "    log_interval = 500\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "        log_probs = model(text)\n",
    "        loss = loss_function(log_probs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'At iteration {idx} the loss is {loss:.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing average accuracy on a validation set\n",
    "\n",
    "**Task 7** [7]: Write a function ``get_accuracy`` to compute the average accuracy of the model for a given dataloader.  Your code should iterate through all the examples, for each find the predicted label with the highest probability, and count the number of examples in which this predicted label is correct.  It should then return the average accuracy. Remember that although most batches will have a fixed number of examples (the given batch size), the last batch may have fewer examples.  So you should explicitly count the number of examples in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ## WRITE YOUR CODE BELOW.    \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training, validation, and testing dataloaders\n",
    "\n",
    "Since the original ``AG_NEWS`` has no valid dataset, we split the training\n",
    "dataset into train/valid sets with a split ratio of 0.95 (train) and\n",
    "0.05 (valid). Here we use\n",
    "[``torch.utils.data.dataset.random_split``](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split)\n",
    "function in PyTorch core library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "  \n",
    "train_valid_data, test_data = AG_NEWS()\n",
    "train_valid_data = list(train_valid_data)\n",
    "num_train = int(len(train_valid_data) * 0.95)\n",
    "num_valid = len(train_valid_data) - num_train\n",
    "train_data, valid_data = random_split(\n",
    "    train_valid_data, [num_train, num_valid])\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, \n",
    "                              collate_fn=collate_into_bow)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, \n",
    "                              collate_fn=collate_into_bow)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, \n",
    "                             collate_fn=collate_into_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "EPOCHS = 15 # epoch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3)\n",
    "\n",
    "accuracies=[]\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_an_epoch(train_dataloader, optimizer)\n",
    "    accuracy = get_accuracy(valid_dataloader)\n",
    "    accuracies.append(accuracy)\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print()\n",
    "    print(f'After epoch {epoch} the validation accuracy is {accuracy:.3f}.')\n",
    "    print()\n",
    "    \n",
    "plt.plot(range(1, EPOCHS+1), accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8** [10]: Run the model for a sufficient number of epochs such that the model shows overfitting, and submit a pdf of the plot of accuracy against number of epochs.  Determine the optimal number of epochs to train for.  Write code to estimate the accuracy of your model corresponding to this optimal number of epocs and report this estimated accuracy.\n",
    "\n",
    "**Task 9** [5]: Notice above that both the printed losses and the accuracies keep varying and do not necessary increase or decrease in a steady fashion.  List all the reasons you can think of for this variance in the loss and the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a pre-trained embedding\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/projects/glove/) is set of dense vector representations, or embeddings.  Torchtext has support for GloVe. (It takes several minutes the first time---to download.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# It is best to save GloVe data in a cache to reuse across projects.\n",
    "VECTOR_CACHE_DIR = '/Users/amitabh/mlpp23/.vector_cache'\n",
    "\n",
    "glove = GloVe(name='6B', cache = VECTOR_CACHE_DIR)\n",
    "\n",
    "words = [\"hello\", \"hi\", \"king\", \"president\"]\n",
    "vecs = glove.get_vecs_by_tokens(words)\n",
    "\n",
    "print(vecs.shape)\n",
    "print()\n",
    "for (i, j) in combinations(range(4), 2):\n",
    "    print(words[i], words[j], vecs[i].dot(vecs[j]))\n",
    "print()\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 10** [10]: Write a new collate function ``collate_into_cbow`` that returns a CBoW representation of each batch, using GloVe.\n",
    "\n",
    "**Task 11** [5]: Write copies of other functions as needed to determine the estimate accuracy of the (optimal) model that incorporates GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Adam optimizer\n",
    "\n",
    "The [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) is usually preferred to SGD because of better convergence properites.\n",
    "\n",
    "**Task 12** [5]: Write copies of functions as needed to plot the convergence of the Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Optimizations\n",
    "\n",
    "**Task 13** [5]: Briefly desribe 3 ways by which you could make the above code run faster or improve its accuracy.  (You don't have to implement your suggestions.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d279945bbc2f7ec20c9acf85309ca93419ae763a19ad62836161601988da241d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
