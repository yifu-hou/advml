{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Introduction to Torch's tensor library\n",
    "======================================\n",
    "\n",
    "All of deep learning is computations on tensors, which are\n",
    "generalizations of a matrix that can be indexed in more than 2\n",
    "dimensions. We will see exactly what this means in-depth later. First,\n",
    "lets look what we can do with tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Author: Robert Guthrie.\n",
    "# Adapted by Amitabh Chaudhary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Tensors\n",
    "\n",
    "Tensors can be created from Python lists with the torch.tensor()\n",
    "function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(data) creates a torch.Tensor object with the given data.\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.tensor(V_data)\n",
    "print(V)\n",
    "\n",
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a 3D tensor anyway? Think about it like this. If you have a\n",
    "vector, indexing into the vector gives you a scalar. If you have a\n",
    "matrix, indexing into the matrix gives you a vector. If you have a 3D\n",
    "tensor, then indexing into the tensor gives you a matrix!\n",
    "\n",
    "A note on terminology:\n",
    "when I say \"tensor\" in this tutorial, it refers\n",
    "to any torch.Tensor object. Matrices and vectors are special cases of\n",
    "torch.Tensors, where their dimension is 1 and 2 respectively. When I am\n",
    "talking about 3D tensors, I will explicitly use the term \"3D tensor\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "# Get a Python number from it\n",
    "print(V[0].item())\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])\n",
    "\n",
    "# Index into T to get the last column of the second matrix.\n",
    "print(T[1,:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create tensors of other data types. To create a tensor of integer types, try\n",
    "torch.tensor([[1, 2], [3, 4]]) (where all elements in the list are integers).\n",
    "You can also specify a data type by passing in ``dtype=torch.data_type``.\n",
    "Check the documentation for more data types, but\n",
    "Float and Long will be the most common.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a tensor with random data and the supplied dimensionality\n",
    "with torch.randn()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with Tensors\n",
    "\n",
    "\n",
    "You can operate on tensors in the ways you would expect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the documentation---[pytorch.org/docs/torch.html](https://pytorch.org/docs/torch.html)---for a\n",
    "complete list of the large number of operations available to you. They\n",
    "expand beyond just mathematical operations.\n",
    "\n",
    "One helpful operation that we will make use of later is concatenation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "#torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the shape of the tensor use either the attribute .shape (with no parentheses following it) or the function .size() (with parentheses since this is a function call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z_2.shape) #.shape is an attribute\n",
    "print(z_2.size()) #.size() is a member functionm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping Tensors\n",
    "\n",
    "\n",
    "Use the .view() method to reshape a tensor. This method is used frequently, because many neural network components expect their inputs in a certain shape. You will often need to reshape your tensor before passing it to a component.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newer function .reshape() is similar to .view(), and actually more general.  There is a [slight difference](https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/) between the two, which is unimportant at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "print(x.reshape(2, 6))\n",
    "print(x.view(2, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can create new tensors that retain the shape and datatype of a given tensor.  Two such functions are ones_like() and rand_like()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones_like(x)\n",
    "print(y)\n",
    "print(torch.rand_like(x.reshape(-1,6))) #-1 implies \"infer the size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply matrices together using the @ operator or the matmul() function.  These will give an error if the sizes don't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [0, 0, 1]])\n",
    "y = torch.tensor([[4, 5]])\n",
    "z = torch.tensor([[1], [2], [3]])\n",
    "\n",
    "print(y @ x)\n",
    "print(y.matmul(x))\n",
    "print(x.matmul(z))\n",
    "print(x @ z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For matrices of compatible sizes use * or mul() to compute an element-wise product. These follow [broadcasting rules](https://numpy.org/doc/stable/user/basics.broadcasting.html) as in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = torch.tensor([[1, 1, 3], [0, 0, 1]])\n",
    "z = torch.tensor([[2],[3]])\n",
    "u = torch.tensor([1, 3, 0])\n",
    "print(x * y)\n",
    "print(y.mul(x))\n",
    "print(x * z)\n",
    "print(x.mul(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the attribute .T or the function transpose(0, 1) to transpose a matrix.  transpose(dim1, dim2) can switch between any two dimensions of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x.T)\n",
    "print(x.transpose(0,1))\n",
    "\n",
    "y = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9],[10, 11, 12]]])\n",
    "print(y)\n",
    "print(y.transpose(0, 1))\n",
    "print(y.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often need to remove a dimension (of size 1) or add a dimension (of size 1) from a tensor.  For these use squeeze() and unsqueeze()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x, x.shape)\n",
    "y = x.unsqueeze(1)\n",
    "print(y, y.shape)\n",
    "z = y[:,:,1]\n",
    "print(z, z.shape)\n",
    "u = z.squeeze(1)\n",
    "print(u, u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PROBLEM 4 (a)\n",
    "\n",
    "Write a function repeat() that takes a tensor of any shape and \"repeats\"\n",
    "the vectors in the last dimension.\n",
    "So if x is tensor([2, 3]), repeat(x) should return\n",
    "    tensor([[2, 3],\n",
    "            [2, 3]])\n",
    "\n",
    "And if x is tensor([[[2, 3],[4,5]],[[7, 8],[9,10]]]),\n",
    "repeat(x) should return\n",
    "    tensor([[[[ 2,  3],\n",
    "              [ 2,  3]],\n",
    "             [[ 4,  5],\n",
    "              [ 4,  5]]],\n",
    "            [[[ 7,  8],\n",
    "              [ 7,  8]],\n",
    "             [[ 9, 10],\n",
    "              [ 9, 10]]]]).\n",
    "Use only the functions torch.cat() and torch.unsqueeze() and no loops.\n",
    "'''\n",
    "\n",
    "def repeat(x):\n",
    "    ## WRITE YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def test_repeat():\n",
    "    x1 = torch.tensor([2, 3])\n",
    "    x2 = torch.tensor([[2, 3],[1,5]])\n",
    "    x3 = torch.tensor([[[2, 3],[4,5]],[[7, 8],[9,10]]])\n",
    "    y1 = torch.tensor([[2, 3], [2, 3]])\n",
    "    y2 = torch.tensor([[[2, 3],[2, 3]],[[1, 5],[1, 5]]]) \n",
    "    y3 = torch.tensor([[[[ 2,  3],[ 2,  3]],[[ 4,  5],[ 4,  5]]],\n",
    "            [[[ 7,  8],[ 7,  8]],[[ 9, 10],[ 9, 10]]]])\n",
    "    assert(torch.equal(repeat(x1),y1))\n",
    "    assert(torch.equal(repeat(x2),y2))    \n",
    "    assert(torch.equal(repeat(x3),y3))\n",
    "    print('Passed all tests.')\n",
    "test_repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning building blocks: affine maps, softmax, and embeddings\n",
    "==========================================================================\n",
    "\n",
    "Deep learning consists of composing linearities with non-linearities in\n",
    "clever ways. The introduction of non-linearities allows for powerful\n",
    "models. In this section, we will learn about the affine map, which is a linearity, and softmax, which is a non-linearity.  We'll also learn about a common objective or loss function used with softmax: negative log likelihood loss.\n",
    "\n",
    "\n",
    "#### Affine Maps\n",
    "\n",
    "\n",
    "One of the core workhorses of deep learning is the affine map, which is\n",
    "a function $f(x)$ where\n",
    "$$\n",
    "\\begin{align}f(x) = Ax + b\\end{align}\n",
    "$$\n",
    "for a matrix $A$ and vectors $x, b$. The parameters to be\n",
    "learned here are $A$ and $b$. Often, $b$ is refered to\n",
    "as the *bias* term.\n",
    "\n",
    "\n",
    "PyTorch and most other deep learning frameworks do things a little\n",
    "differently than traditional linear algebra. It maps the rows of the\n",
    "input instead of the columns. That is, the $i$'th row of the\n",
    "output below is the mapping of the $i$'th row of the input under\n",
    "$A$, plus the bias term. Look at the example below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = nn.Linear(3, 2)\n",
    "# The linear layer gets initialized with random parameters\n",
    "# corresponding to A and b.\n",
    "print(\"The weight parameter\")\n",
    "print(lin.weight)\n",
    "print(\"The bias parameter\")\n",
    "print(lin.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us change these parameters to simpler numbers\n",
    "with torch.no_grad():    \n",
    "    lin.weight.copy_(torch.tensor([[3.0, 4.0, 3.0], \n",
    "                                   [3.0, 4.0, 1.0]]))\n",
    "    lin.bias.copy_(torch.tensor([1.0, 2.5]))\n",
    "# Let the data be 2 x 3 matrix.  The linear layer will \n",
    "# transform each row x to f(x)\n",
    "data = torch.tensor([[4.0, 6.0, 1.0], [7.0, 1.0, 0.0]])\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax and Probabilities\n",
    "\n",
    "The function  Softmax(𝑥) is a non-linearity.  It is usually the last operation done in a network. This is because it takes in a vector of real numbers and returns a probability distribution. Its definition is as follows. Let  $x$  be a vector of real numbers (in $[-\\infty, \\infty]$). Then the i'th component of  Softmax(𝑥)  is\n",
    "\n",
    "$$ \\frac{\\exp(𝑥_i)}{\\sum_j \\exp(x_j)}$$\n",
    " \n",
    "It should be clear that the output is a probability distribution: each element is non-negative and the sum over all components is $1$.\n",
    "\n",
    "You could also think of it as just applying an element-wise exponentiation operator to the input to make everything non-negative and then dividing by the normalization constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax is also in torch.nn.functional\n",
    "data = torch.tensor([3.0, 4.0, 5.0])\n",
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PROBLEM 4 (b)\n",
    "\n",
    "Write a function mysoftmax() that takes a vector, and applies\n",
    "softmax to it.  The returned vector consists of probabilities that \n",
    "add to 1.\n",
    "\n",
    "So mysoftmax(torch.tensor([3.0,4.0, 5.0,-600.0])) returns\n",
    "    tensor([0.0900, 0.2447, 0.6652, 0.0000]).\n",
    "'''\n",
    "\n",
    "def mysoftmax(x):\n",
    "    ## WRITE YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def test_mysoftmax():\n",
    "    x1 = torch.tensor([3.0,4.0,5.0,-600.0])\n",
    "    x2 = torch.tensor([-33.44,4.44,-5.01,-6.0])\n",
    "    assert(torch.allclose(mysoftmax(x1), F.softmax(x1,0)))\n",
    "    assert(torch.allclose(mysoftmax(x2), F.softmax(x2,0)))\n",
    "    print('Passed all tests.')\n",
    "\n",
    "test_mysoftmax()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PROBLEM 4(c)\n",
    "\n",
    "Extend above mysoftmax to accept tensors of any shape\n",
    "Write a function mysoftmaxex() that takes a tensor of any shape, and\n",
    "a dimension d, and applies softmax along dimension d.  So slices \n",
    "along dimension d consist of probabilities that add to 1.\n",
    "E.g., mysoftmaxex(tensor([[3.0,4.0, 2.3],[5.0,-600.0, 2.3]]),0)\n",
    "returns\n",
    "    tensor([[0.1192, 1.0000, 0.5000],\n",
    "            [0.8808, 0.0000, 0.5000]])\n",
    "and mysoftmaxex(tensor([[3.0,4.0, 2.3],[5.0,-600.0, 2.3]]),1)\n",
    "returns\n",
    "    tensor([[0.2373, 0.6449, 0.1178],\n",
    "            [0.9370, 0.0000, 0.0630]]).\n",
    "'''\n",
    "\n",
    "def mysoftmaxex(x, d):\n",
    "    ## WRITE YOUR CODE HERE.\n",
    "    ## You will want to use torch.exp() \n",
    "    ## https://pytorch.org/docs/stable/generated/torch.exp.html\n",
    "    ## and torch.sum()\n",
    "    ## https://pytorch.org/docs/stable/generated/torch.sum.html\n",
    "    pass\n",
    "\n",
    "def test_mysoftmaxex():\n",
    "    x = torch.tensor([[3.0,4.0, 2.3],[5.0,-600.0, 2.3]])\n",
    "    assert(torch.allclose(mysoftmaxex(x,0), F.softmax(x,0)))\n",
    "    assert(torch.allclose(mysoftmaxex(x,1), F.softmax(x,1)))\n",
    "test_mysoftmaxex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "[Torchtext](https://pytorch.org/text/stable/index.html) is a library within the PyTorch framework that consists of data processing utilities and popular datasets for natural language processing.\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/projects/glove/) is set of dense vector representations, or embeddings.  Torchtext has support for GloVe. (The following code takes several minutes to run the first time, since it downloads the GloVe embeddings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "glove = GloVe(name='6B')\n",
    "\n",
    "words = [\"hello\", \"hi\", \"king\", \"president\"]\n",
    "vecs = glove.get_vecs_by_tokens(words)\n",
    "\n",
    "print(vecs.shape)\n",
    "print('The first 10 values in the embedding for \"hello\" are',\n",
    "     vecs[0,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PROBLEM 4(d)\n",
    "\n",
    "Write code to verify if in GloVe \"similar words map into \n",
    "similar vectors.  Briefly discuss your results.\"\n",
    "'''\n",
    "\n",
    "## WRITE YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
