{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 - Ambiguity in English\n",
    "\n",
    "#### (a) Lexical ambiguity\n",
    "\n",
    "**Example:** ***\"You will see lots of plants in the south of the city.\"*** </p>\n",
    "**Explanation:** The word \"plant\" could mean either greenery including trees, flowers and herbs, or it could mean industrial facilities such as power plants. \n",
    "\n",
    "#### (b) Syntactic ambiguity\n",
    "\n",
    "**Example:** ***\"She stared at a girl in a blue dress\"*** </p>\n",
    "**Explanation:** \"She\" could be staring a girl who wears a blue dress, or she could be in a blue dress while staring at a girl.\n",
    "\n",
    "#### (c) Semantic ambiguity\n",
    "\n",
    "**Example:** ***\"I took off Joe's jacket.\"*** </p>\n",
    "**Explanation:** The sentence could suggest two different actions: taking Joe's jacket off myself or taking the jacket off Joe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 - Validation\n",
    "\n",
    "Let $U$ be all the data possible, $H$ be the set of hypothesis set that includes all the functions $f$, $f^*$ be the function that minimizes the error, $A$ be the algorithm to find $f^*$.</p>\n",
    "\n",
    "##### Statement 1: \n",
    "_An unbiased estimate of the generalized error rate for a decision tree with max depth 5, trained on T , is 0.35._ </p>\n",
    "\n",
    "**Answer:** </p>\n",
    "False. Let the solution to this training set $T$ be $f^A_T$. In this case, both the training set and the validation set $V$ are i.i.d. samples of all the data $U$. However, since there are limitless possible solutions to find a decision tree with max depth 5, $ f^A_T \\neq f^* $. So 0.35 is not an unbiased estimate of the generalized error rate for a decision tree with max depth 5, trained on T.\n",
    "\n",
    "##### Statement 2:\n",
    "_An unbiased estimate of the generalized error rate for a tuned decision tree, trained on T , is 0.35._ </p>\n",
    "\n",
    "**Answer:** </p>\n",
    "True. By definition given from the question, a ***tuned decision tree*** is a decision tree chosen from the three decision trees trained on $T$ for which the validation error rate is minimum. Since we specified the training set $T$, the number of optimal models with each depth is fixed. And because our validation set $V$ is i.i.d. sample of $U$ ( assume it has a size of $m$), the expected value of average test error can be written as:\n",
    "\n",
    "$ E[avg test error] = {1 \\over m} \\sum_{(x_i, y_i) \\in S} E[error(x_i, y_i, f^A_T)] $\n",
    "\n",
    "The value of $ E[avg test error] $ doesn't rely on $i$, which means that it is an unbiased estimate of the deneralization error rate for a ***tuned decision tree***.\n",
    "\n",
    "##### Statement 3:\n",
    "_An unbiased estimate of the generalized error rate of the random decision tree is 0.35._ </p>\n",
    "\n",
    "**Answer:** </p>\n",
    "\n",
    "False. Generalization error rate refers to the expected value of the misclassification error over **all** possible examples for the task. Since we defined random decision tree trained on $T$ to be either one model from all 3 possible models, the generalization error rate should be calculated across 3 different models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 - Classification Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a)\n",
    "\n",
    "By definition, logistic regression is written as: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(x) = {1 \\over {1 + e^{ \\mathbf{w^T}x+w_0}}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When applied for classification problem for a single class, with $0$ and $1$ as the binary indicator, we usually set the decision boundary of logistic regression to 0.5, so we have $ p(x) = {1 \\over 2} $.\n",
    "\n",
    "From \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(x) = {1 \\over 2} = {1 \\over {1 + e^{ \\mathbf{w^T}x+w_0}}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "we get: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "e^{ \\mathbf{w^T}x+w_0} = 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{w^T}x+w_0 = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By definition, $ \\mathbf{w}^T \\mathbf{x} + w_0 = 0 $ is a linear model for classification.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (b)\n",
    "\n",
    "Any such classifier is a linear model. Reason: </p>\n",
    "\n",
    "Suppose there are $K$ classes $C_1, C_2, \\dots C_K$. $ 0 \\leq P(C_k | x) \\leq 1 $, $ 0 \\leq P(C_j | x) \\leq 1 $ and $k \\neq j$. </p>\n",
    "\n",
    "The decision boundary between any class $k$ and $j$ is found when $ P(C_k | x) = P(C_j | x)$. </p>\n",
    "\n",
    "Since for any class $C_K$, $ P(C_k | x) = f(\\mathbf{w_k}^T \\mathbf{x} + w_{k,0}) $, assume $ k > j $, the equation for boundary between $C_K$ and $C_J$ can be converted into: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\mathbf{w_k}^T \\mathbf{x} + w_{k,0}) = f(\\mathbf{w_j}^T \\mathbf{x} + w_{j,0})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Because non-linear function $f$ is an one-to-one function,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{w_k}^T \\mathbf{x} + w_{k,0} = \\mathbf{w_j}^T \\mathbf{x} + w_{j,0}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(\\mathbf{w_k - w_j})^T \\mathbf{x} + (w_{k,0} - w_{j,0}) = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Again, since $k \\neq j$, this equation can be simplified as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{w_m}^T \\mathbf{x} + w_{m,0} = 0 \\space , \\space where \\space m = k - j\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By definition, the classifier for multi-class classification with logistic regression is a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Introduction to Torch's tensor library\n",
    "======================================\n",
    "\n",
    "All of deep learning is computations on tensors, which are\n",
    "generalizations of a matrix that can be indexed in more than 2\n",
    "dimensions. We will see exactly what this means in-depth later. First,\n",
    "lets look what we can do with tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6ed46497d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original Author: Robert Guthrie.\n",
    "# Adapted by Amitabh Chaudhary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Tensors\n",
    "\n",
    "Tensors can be created from Python lists with the torch.tensor()\n",
    "function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor(data) creates a torch.Tensor object with the given data.\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.tensor(V_data)\n",
    "print(V)\n",
    "\n",
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a 3D tensor anyway? Think about it like this. If you have a\n",
    "vector, indexing into the vector gives you a scalar. If you have a\n",
    "matrix, indexing into the matrix gives you a vector. If you have a 3D\n",
    "tensor, then indexing into the tensor gives you a matrix!\n",
    "\n",
    "A note on terminology:\n",
    "when I say \"tensor\" in this tutorial, it refers\n",
    "to any torch.Tensor object. Matrices and vectors are special cases of\n",
    "torch.Tensors, where their dimension is 1 and 2 respectively. When I am\n",
    "talking about 3D tensors, I will explicitly use the term \"3D tensor\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n",
      "tensor([1., 2., 3.])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([6., 8.])\n"
     ]
    }
   ],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "# Get a Python number from it\n",
    "print(V[0].item())\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])\n",
    "\n",
    "# Index into T to get the last column of the second matrix.\n",
    "print(T[1,:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create tensors of other data types. To create a tensor of integer types, try\n",
    "torch.tensor([[1, 2], [3, 4]]) (where all elements in the list are integers).\n",
    "You can also specify a data type by passing in ``dtype=torch.data_type``.\n",
    "Check the documentation for more data types, but\n",
    "Float and Long will be the most common.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a tensor with random data and the supplied dimensionality\n",
    "with torch.randn()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002],\n",
      "         [-0.6092, -0.9798, -1.6091, -0.7121,  0.3037],\n",
      "         [-0.7773, -0.2515, -0.2223,  1.6871,  0.2284],\n",
      "         [ 0.4676, -0.6970, -1.1608,  0.6995,  0.1991]],\n",
      "\n",
      "        [[ 0.8657,  0.2444, -0.6629,  0.8073,  1.1017],\n",
      "         [-0.1759, -2.2456, -1.4465,  0.0612, -0.6177],\n",
      "         [-0.7981, -0.1316,  1.8793, -0.0721,  0.1578],\n",
      "         [-0.7735,  0.1991,  0.0457,  0.1530, -0.4757]],\n",
      "\n",
      "        [[-0.1110,  0.2927, -0.1578, -0.0288,  0.4533],\n",
      "         [ 1.1422,  0.2486, -1.7754, -0.0255, -1.0233],\n",
      "         [-0.5962, -1.0055,  0.4285,  1.4761, -1.7869],\n",
      "         [ 1.6103, -0.7040, -0.1853, -0.9962, -0.8313]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations with Tensors\n",
    "\n",
    "\n",
    "You can operate on tensors in the ways you would expect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the documentation---[pytorch.org/docs/torch.html](https://pytorch.org/docs/torch.html)---for a\n",
    "complete list of the large number of operations available to you. They\n",
    "expand beyond just mathematical operations.\n",
    "\n",
    "One helpful operation that we will make use of later is concatenation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8029,  0.2366,  0.2857,  0.6898, -0.6331],\n",
      "        [ 0.8795, -0.6842,  0.4533,  0.2912, -0.8317],\n",
      "        [-0.5525,  0.6355, -0.3968, -0.6571, -1.6428],\n",
      "        [ 0.9803, -0.0421, -0.8206,  0.3133, -1.1352],\n",
      "        [ 0.3773, -0.2824, -2.5667, -1.4303,  0.5009]])\n",
      "tensor([[ 0.5438, -0.4057,  1.1341, -0.1473,  0.6272,  1.0935,  0.0939,  1.2381],\n",
      "        [-1.1115,  0.3501, -0.7703, -1.3459,  0.5119, -0.6933, -0.1668, -0.9999]])\n"
     ]
    }
   ],
   "source": [
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "#torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the shape of the tensor use either the attribute .shape (with no parentheses following it) or the function .size() (with parentheses since this is a function call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "print(z_2.shape) #.shape is an attribute\n",
    "print(z_2.size()) #.size() is a member functionm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping Tensors\n",
    "\n",
    "\n",
    "Use the .view() method to reshape a tensor. This method is used frequently, because many neural network components expect their inputs in a certain shape. You will often need to reshape your tensor before passing it to a component.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4175, -0.2127, -0.8400, -0.4200],\n",
      "         [-0.6240, -0.9773,  0.8748,  0.9873],\n",
      "         [-0.0594, -2.4919,  0.2423,  0.2883]],\n",
      "\n",
      "        [[-0.1095,  0.3126,  1.5038,  0.5038],\n",
      "         [ 0.6223, -0.4481, -0.2856,  0.3880],\n",
      "         [-1.1435, -0.6512, -0.1032,  0.6937]]])\n",
      "tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,  0.9873,\n",
      "         -0.0594, -2.4919,  0.2423,  0.2883],\n",
      "        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,  0.3880,\n",
      "         -1.1435, -0.6512, -0.1032,  0.6937]])\n",
      "tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,  0.9873,\n",
      "         -0.0594, -2.4919,  0.2423,  0.2883],\n",
      "        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,  0.3880,\n",
      "         -1.1435, -0.6512, -0.1032,  0.6937]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newer function .reshape() is similar to .view(), and actually more general.  There is a [slight difference](https://jdhao.github.io/2019/07/10/pytorch_view_reshape_transpose_permute/) between the two, which is unimportant at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6476,  1.0156, -0.2020, -1.2865],\n",
      "        [ 0.8231, -0.6101, -1.2960, -0.9434],\n",
      "        [ 0.6684,  1.1628, -0.3229,  1.8782]])\n",
      "tensor([[-1.6476,  1.0156, -0.2020, -1.2865,  0.8231, -0.6101],\n",
      "        [-1.2960, -0.9434,  0.6684,  1.1628, -0.3229,  1.8782]])\n",
      "tensor([[-1.6476,  1.0156, -0.2020, -1.2865,  0.8231, -0.6101],\n",
      "        [-1.2960, -0.9434,  0.6684,  1.1628, -0.3229,  1.8782]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "print(x.reshape(2, 6))\n",
    "print(x.view(2, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can create new tensors that retain the shape and datatype of a given tensor.  Two such functions are ones_like() and rand_like()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0.7444, 0.1408, 0.3854, 0.8637, 0.8960, 0.9729],\n",
      "        [0.3985, 0.1114, 0.9923, 0.3935, 0.2943, 0.6219]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.ones_like(x)\n",
    "print(y)\n",
    "print(torch.rand_like(x.reshape(-1,6))) #-1 implies \"infer the size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply matrices together using the @ operator or the matmul() function.  These will give an error if the sizes don't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  8, 17]])\n",
      "tensor([[ 4,  8, 17]])\n",
      "tensor([[14],\n",
      "        [ 3]])\n",
      "tensor([[14],\n",
      "        [ 3]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [0, 0, 1]])\n",
    "y = torch.tensor([[4, 5]])\n",
    "z = torch.tensor([[1], [2], [3]])\n",
    "\n",
    "print(y @ x)\n",
    "print(y.matmul(x))\n",
    "print(x.matmul(z))\n",
    "print(x @ z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For matrices of compatible sizes use * or mul() to compute an element-wise product. These follow [broadcasting rules](https://numpy.org/doc/stable/user/basics.broadcasting.html) as in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 9],\n",
      "        [0, 0, 6]])\n",
      "tensor([[1, 2, 9],\n",
      "        [0, 0, 6]])\n",
      "tensor([[ 2,  4,  6],\n",
      "        [12, 15, 18]])\n",
      "tensor([[ 1,  6,  0],\n",
      "        [ 4, 15,  0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = torch.tensor([[1, 1, 3], [0, 0, 1]])\n",
    "z = torch.tensor([[2],[3]])\n",
    "u = torch.tensor([1, 3, 0])\n",
    "print(x * y)\n",
    "print(y.mul(x))\n",
    "print(x * z)\n",
    "print(x.mul(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the attribute .T or the function transpose(0, 1) to transpose a matrix.  transpose(dim1, dim2) can switch between any two dimensions of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 7,  8,  9]],\n",
      "\n",
      "        [[ 4,  5,  6],\n",
      "         [10, 11, 12]]])\n",
      "tensor([[[ 1,  4],\n",
      "         [ 2,  5],\n",
      "         [ 3,  6]],\n",
      "\n",
      "        [[ 7, 10],\n",
      "         [ 8, 11],\n",
      "         [ 9, 12]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x.T)\n",
    "print(x.transpose(0,1))\n",
    "\n",
    "y = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9],[10, 11, 12]]])\n",
    "print(y)\n",
    "print(y.transpose(0, 1))\n",
    "print(y.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often need to remove a dimension (of size 1) or add a dimension (of size 1) from a tensor.  For these use squeeze() and unsqueeze()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]]) torch.Size([2, 3])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "tensor([[2],\n",
      "        [5]]) torch.Size([2, 1])\n",
      "tensor([2, 5]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x, x.shape)\n",
    "y = x.unsqueeze(1)\n",
    "print(y, y.shape)\n",
    "z = y[:,:,1]\n",
    "print(z, z.shape)\n",
    "u = z.squeeze(1)\n",
    "print(u, u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all tests.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "PROBLEM 4 (a)\n",
    "\n",
    "Write a function repeat() that takes a tensor of any shape and \"repeats\"\n",
    "the vectors in the last dimension.\n",
    "So if x is tensor([2, 3]), repeat(x) should return\n",
    "    tensor([[2, 3],\n",
    "            [2, 3]])\n",
    "\n",
    "And if x is tensor([[[2, 3],[4,5]],[[7, 8],[9,10]]]),\n",
    "repeat(x) should return\n",
    "    tensor([[[[ 2,  3],\n",
    "              [ 2,  3]],\n",
    "             [[ 4,  5],\n",
    "              [ 4,  5]]],\n",
    "            [[[ 7,  8],\n",
    "              [ 7,  8]],\n",
    "             [[ 9, 10],\n",
    "              [ 9, 10]]]]).\n",
    "Use only the functions torch.cat() and torch.unsqueeze() and no loops.\n",
    "'''\n",
    "\n",
    "def repeat(x):\n",
    "    '''\n",
    "    Repeat the last dimension of a torch object\n",
    "    '''\n",
    "    new_shape = list(x.shape)\n",
    "    new_shape.append(new_shape[-1])\n",
    "    new_shape[-2] = 2\n",
    "    return torch.cat([x, x], len(x.shape) - 1).reshape(new_shape)\n",
    "\n",
    "def test_repeat():\n",
    "    x1 = torch.tensor([2, 3])\n",
    "    x2 = torch.tensor([[2, 3],[1,5]])\n",
    "    x3 = torch.tensor([[[2, 3],[4,5]],[[7, 8],[9,10]]])\n",
    "    y1 = torch.tensor([[2, 3], [2, 3]])\n",
    "    y2 = torch.tensor([[[2, 3],[2, 3]],[[1, 5],[1, 5]]]) \n",
    "    y3 = torch.tensor([[[[ 2,  3],[ 2,  3]],[[ 4,  5],[ 4,  5]]],\n",
    "            [[[ 7,  8],[ 7,  8]],[[ 9, 10],[ 9, 10]]]])\n",
    "    assert(torch.equal(repeat(x1),y1))\n",
    "    assert(torch.equal(repeat(x2),y2))    \n",
    "    assert(torch.equal(repeat(x3),y3))\n",
    "    print('Passed all tests.')\n",
    "test_repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning building blocks: affine maps, softmax, and embeddings\n",
    "==========================================================================\n",
    "\n",
    "Deep learning consists of composing linearities with non-linearities in\n",
    "clever ways. The introduction of non-linearities allows for powerful\n",
    "models. In this section, we will learn about the affine map, which is a linearity, and softmax, which is a non-linearity.  We'll also learn about a common objective or loss function used with softmax: negative log likelihood loss.\n",
    "\n",
    "\n",
    "#### Affine Maps\n",
    "\n",
    "\n",
    "One of the core workhorses of deep learning is the affine map, which is\n",
    "a function $f(x)$ where\n",
    "$$\n",
    "\\begin{align}f(x) = Ax + b\\end{align}\n",
    "$$\n",
    "for a matrix $A$ and vectors $x, b$. The parameters to be\n",
    "learned here are $A$ and $b$. Often, $b$ is refered to\n",
    "as the *bias* term.\n",
    "\n",
    "\n",
    "PyTorch and most other deep learning frameworks do things a little\n",
    "differently than traditional linear algebra. It maps the rows of the\n",
    "input instead of the columns. That is, the $i$'th row of the\n",
    "output below is the mapping of the $i$'th row of the input under\n",
    "$A$, plus the bias term. Look at the example below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight parameter\n",
      "Parameter containing:\n",
      "tensor([[-0.4038,  0.3795,  0.3618],\n",
      "        [-0.4581, -0.4742, -0.0506]], requires_grad=True)\n",
      "The bias parameter\n",
      "Parameter containing:\n",
      "tensor([ 0.2425, -0.0167], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(3, 2)\n",
    "# The linear layer gets initialized with random parameters\n",
    "# corresponding to A and b.\n",
    "print(\"The weight parameter\")\n",
    "print(lin.weight)\n",
    "print(\"The bias parameter\")\n",
    "print(lin.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[40.0000, 39.5000],\n",
      "        [26.0000, 27.5000]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let us change these parameters to simpler numbers\n",
    "with torch.no_grad():    \n",
    "    lin.weight.copy_(torch.tensor([[3.0, 4.0, 3.0], \n",
    "                                   [3.0, 4.0, 1.0]]))\n",
    "    lin.bias.copy_(torch.tensor([1.0, 2.5]))\n",
    "# Let the data be 2 x 3 matrix.  The linear layer will \n",
    "# transform each row x to f(x)\n",
    "data = torch.tensor([[4.0, 6.0, 1.0], [7.0, 1.0, 0.0]])\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax and Probabilities\n",
    "\n",
    "The function  Softmax(ùë•) is a non-linearity.  It is usually the last operation done in a network. This is because it takes in a vector of real numbers and returns a probability distribution. Its definition is as follows. Let  $x$  be a vector of real numbers (in $[-\\infty, \\infty]$). Then the i'th component of  Softmax(ùë•)  is\n",
    "\n",
    "$$ \\frac{\\exp(ùë•_i)}{\\sum_j \\exp(x_j)}$$\n",
    " \n",
    "It should be clear that the output is a probability distribution: each element is non-negative and the sum over all components is $1$.\n",
    "\n",
    "You could also think of it as just applying an element-wise exponentiation operator to the input to make everything non-negative and then dividing by the normalization constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n",
      "tensor(1.)\n",
      "tensor([-2.4076, -1.4076, -0.4076])\n"
     ]
    }
   ],
   "source": [
    "# Softmax is also in torch.nn.functional\n",
    "data = torch.tensor([3.0, 4.0, 5.0])\n",
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all tests.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "PROBLEM 4 (b)\n",
    "\n",
    "Write a function mysoftmax() that takes a vector, and applies\n",
    "softmax to it.  The returned vector consists of probabilities that \n",
    "add to 1.\n",
    "\n",
    "So mysoftmax(torch.tensor([3.0,4.0, 5.0,-600.0])) returns\n",
    "    tensor([0.0900, 0.2447, 0.6652, 0.0000]).\n",
    "'''\n",
    "\n",
    "def mysoftmax(x):\n",
    "    '''\n",
    "    Apply softmax to a vector \n",
    "    '''\n",
    "    expons = torch.exp(x)\n",
    "    return expons/torch.sum(expons)\n",
    "\n",
    "\n",
    "def test_mysoftmax():\n",
    "    x1 = torch.tensor([3.0,4.0,5.0,-600.0])\n",
    "    x2 = torch.tensor([-33.44,4.44,-5.01,-6.0])\n",
    "    assert(torch.allclose(mysoftmax(x1), F.softmax(x1,0)))\n",
    "    assert(torch.allclose(mysoftmax(x2), F.softmax(x2,0)))\n",
    "    print('Passed all tests.')\n",
    "\n",
    "test_mysoftmax()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PROBLEM 4(c)\n",
    "\n",
    "Extend above mysoftmax to accept tensors of any shape\n",
    "Write a function mysoftmaxex() that takes a tensor of any shape, and\n",
    "a dimension d, and applies softmax along dimension d.  So slices \n",
    "along dimension d consist of probabilities that add to 1.\n",
    "E.g., mysoftmaxex(tensor([[3.0,4.0, 2.3],[5.0,-600.0, 2.3]]),0)\n",
    "returns\n",
    "    tensor([[0.1192, 1.0000, 0.5000],\n",
    "            [0.8808, 0.0000, 0.5000]])\n",
    "and mysoftmaxex(tensor([[3.0,4.0, 2.3],[5.0,-600.0, 2.3]]),1)\n",
    "returns\n",
    "    tensor([[0.2373, 0.6449, 0.1178],\n",
    "            [0.9370, 0.0000, 0.0630]]).\n",
    "'''\n",
    "\n",
    "def mysoftmaxex(x, d):\n",
    "    '''\n",
    "    Apply softmax function mysoftmax() by dimension d\n",
    "    '''\n",
    "    dim_num = x.dim() - 1 - d\n",
    "    mylist = [mysoftmax(x_i) for x_i in torch.unbind(x, dim=dim_num)]\n",
    "    return torch.stack(mylist, dim=dim_num)\n",
    "\n",
    "def test_mysoftmaxex():\n",
    "    x = torch.tensor([[3.0,4.0, 2.3],[5.0,-600.0, 2.3]])\n",
    "    assert(torch.allclose(mysoftmaxex(x,0), F.softmax(x,0)))\n",
    "    assert(torch.allclose(mysoftmaxex(x,1), F.softmax(x,1)))\n",
    "test_mysoftmaxex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "[Torchtext](https://pytorch.org/text/stable/index.html) is a library within the PyTorch framework that consists of data processing utilities and popular datasets for natural language processing.\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/projects/glove/) is set of dense vector representations, or embeddings.  Torchtext has support for GloVe. (The following code takes several minutes to run the first time, since it downloads the GloVe embeddings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 300])\n",
      "The first 10 values in the embedding for \"hello\" are tensor([-0.3371, -0.2169, -0.0066, -0.4162, -1.2555, -0.0285, -0.7219, -0.5289,\n",
      "         0.0072,  0.3200])\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "glove = GloVe(name='6B')\n",
    "\n",
    "words = [\"hello\", \"hi\", \"king\", \"president\"]\n",
    "vecs = glove.get_vecs_by_tokens(words)\n",
    "\n",
    "print(vecs.shape)\n",
    "print('The first 10 values in the embedding for \"hello\" are',\n",
    "     vecs[0,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between words:\n",
      "hello - hi : 0.33\n",
      "hello - king : 0.053\n",
      "hello - president : 0.06\n",
      "hi - king : 0.03\n",
      "hi - president : -0.053\n",
      "king - president : 0.267\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "PROBLEM 4(d)\n",
    "\n",
    "Write code to verify if in GloVe \"similar words map into \n",
    "similar vectors.  Briefly discuss your results.\"\n",
    "'''\n",
    "\n",
    "# One way to check the similarity between two word embeddings is \n",
    "# to use cosine similarity score as a measurement, which should \n",
    "# range from -1 to 1\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# define cos() for calculating cosine similarity\n",
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "\n",
    "print(\"The cosine similarity between words:\" )\n",
    "\n",
    "for pair in combinations(range(4), 2):\n",
    "    i, j = pair\n",
    "    print(\"{} - {} : {}\".format(words[i], words[j], round(float(cos(vecs[i], vecs[j])), 3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As cosine similarity shows, the closest word embeddings among the possible pairs from 4 given words are: </p>\n",
    "cos(`hello`,`hi`) = 0.33, </p>\n",
    "cos(`king`,`president`) = 0.267 </p>\n",
    "This suggests that in GloVe, these pairs of words are considered similar to each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
